'''
proc_eval_metrics.py

Helper functions for processing evaluation metrics datasets

:author: Guy Litt <guy.litt@noaa.gov>
:description: functions read in yaml schema and standardize metrics datasets
:note: developed using python v3.12

'''
#  Changelog/contributions
#     2024-07-02 Originally created, GL
#     2024-07-09 added different file format/dir path options; add file format checkers, GL
#     2024-08-13 update docstrings, GL

import pandas as pd
from pathlib import Path
import yaml
import xarray as xr
import netCDF4
import warnings
import os
import shutil
from importlib import resources as impresources
from fs_proc import data
from itertools import compress
import pynhd as nhd


def _proc_flatten_ls_of_dict_keys(config: dict, key: str) -> list:
    keys_cs = list()
    for v in config[key]:
        keys_cs.append(list(v.keys()))
    return [x for xs in keys_cs for x in xs]

def _read_std_config():
    """Read the standardized categorical mappings used in fs_proc

    :return: yaml configuration file mappings as a dict of lists of dicts
    :rtype: dict
    """
    catg_file = impresources.files(data) / 'fs_categories.yaml'
    with catg_file.open("rt") as f:
        std_config = yaml.safe_load(f)
    return std_config

def _conv_ls_dicts_df_long(config: dict):
    """transpose the yaml schema's converted dataframe into a long format

    :param config: config file generated by reading in a yaml schema. 
    :type config: dict
    :seealso: :func:`_read_std_config()`
    :return: configuration schema converted to a dataframe
    :rtype: pd.DataFrame
    """
    # Convert dict of lists into pd.DataFrame
    ls_form = list()
    for k, vv in config.items():
        for v in vv:
            df_keys = pd.DataFrame(v, index = [0])
            df_keys.loc[1] = k # 
            ls_form.append(df_keys)
    df_all = pd.concat(ls_form, axis=1)

    # transpose and rename cols
    dft = df_all.transpose()
    dft.columns = ['description','category']
    dft.reset_index(inplace=True)
    dft = dft.rename(columns={'index':'var'})
    return dft


def _proc_check_input_config(
    config: dict, 
    std_keys=['file_io','col_schema','formulation_metadata','references'],
    req_col_schema=['gage_id', 'metric_cols'],
    req_form_meta=[
        'dataset_name','formulation_base','target_var','start_date', 
        'end_date','cal_status'
        ],
    req_file_io=['dir_save', 'save_type','save_loc']
    ):
    
    """
    Check input config file to ensure it contains the minimum expected 
    |    categories

    :raises ValueError: _description_
    :raises ValueError: _description_
    :raises ValueError: _description_
    :raises ValueError: _description_

    :seealso: :func: `read_schm_ls_of_dict`
    :TODO: add further checks after testing more datasets

    """
    # Expected standard keys:
    chck_dict = {key: config[key] for key in std_keys}
    if len(chck_dict) != len(std_keys):
        raise ValueError("The provided keys in the input config file"
                        " should include the following:"
                        f" {', '.join(std_keys)}")
    
    # required keys defined inside col_schema
    keys_col_schema = _proc_flatten_ls_of_dict_keys(config, 'col_schema')
    if not all([x in keys_col_schema for x in req_col_schema]):
        raise ValueError("The input config file expects the following"
                        " defined under 'col_schema':"
                        f" {', '.join(req_col_schema)}")

    # required keys defined in formulation_metadata
    keys_form_meta = _proc_flatten_ls_of_dict_keys(config, 'formulation_metadata')
    if not all([x in keys_form_meta for x in req_form_meta]):
        raise ValueError("The input config file expects the following"
                        " defined under 'formulation_metadata':"
                        f" {', '.join(req_form_meta)}")

    # required keys defined in file_io
    keys_file_io = _proc_flatten_ls_of_dict_keys(config, 'file_io')
    if not all([x in keys_file_io for x in req_file_io]):
        raise ValueError(f"The input config file expects the following"
                        f" defined under 'formulation_metadata': {', '.join(req_file_io)}")

def read_schm_ls_of_dict(schema_path: str | os.PathLike) -> pd.DataFrame:
    """Read a dataset's configuration file designed as a list of dicts

    :param schema_path: path to the user-created configuration file
    :type schema_path: str | os.PathLike
    :return: the filepath to the schema
    :rtype: pd.DataFrame

    """
    # Changelog/contributions
    #   2024-07-02 Originally created, GL

    # Load the YAML configuration file
    with open(schema_path, 'r') as file:
        config = yaml.safe_load(file)

    # Run check on expected config formats
    _proc_check_input_config(config)

    # Convert dict of lists into pd.DataFrame
    ls_form = list()
    for k, vv in config.items():

        for v in vv:
            ls_form.append(pd.DataFrame(v, index = [0]))
    df_all = pd.concat(ls_form, axis=1)

    return df_all

def _save_dir_struct(dir_save: str | os.PathLike, 
                        dataset_name: str, 
                        save_type:str ) -> tuple[Path, dict]:
    # Create a standard directory saving structure (in cases of local filesaving)
    save_dir_base = Path(Path(dir_save) / Path('user_data_std') / dataset_name)
    save_dir_base.mkdir(exist_ok=True, parents = True)

    other_save_dirs = dict()
    if save_type == 'csv' or save_type == 'parquet': # For non-hierachical files
        # Otherwise single hierarchical files will be saved in lieu of
        # subdirectories populated w/ .csv files

        # Design dir structure for writing multiple files
        save_dir_attr = Path(save_dir_base / Path('attributes'))
        save_dir_eval_metr = Path(save_dir_base / Path('eval')/Path('metrics'))
        save_dir_eval_ts = Path(save_dir_base / Path('eval')/Path('timeseries'))
        save_dir_meta = Path(save_dir_base / Path('metadata'))
        save_dir_meta_lic = Path(save_dir_meta/Path('license'))
        save_dir_config =  Path(save_dir_base / Path('config'))
        # Generate the expected subdirectories for storing multiple files
        save_dir_attr.mkdir(exist_ok=True, parents = True)
        save_dir_eval_metr.mkdir(exist_ok=True, parents = True)
        save_dir_eval_ts.mkdir(exist_ok=True, parents = True)
        save_dir_meta_lic.mkdir(exist_ok=True, parents = True)
        save_dir_config.mkdir(exist_ok=True, parents = True)
        other_save_dirs = {'attr': save_dir_attr, 
                            'eval_metr': save_dir_eval_metr, 
                            'eval_ts' : save_dir_eval_ts,
                            'meta': save_dir_meta, 
                            'meta_lic': save_dir_meta_lic, 
                            'config': save_dir_meta}

    return save_dir_base, other_save_dirs

def _proc_check_std_fs_ids(vars: list, category=['metric','target_var'][0]):
    """
    Run check to ensure that variables are listed in the standardized 
        fs_categories.yaml

    :param vars: user-defined variable listing of the anticipated mapped
        variables (e.g. ['NSE','RMSE'])
    :type vars: list
    :param category: choose the category of 'metric' or 'target_var' desired
        from the formulation-selector standardized categories file. Defaults to 'metric'
    :type category: list, optional
    :raises ValueError: If at least one of the provided vars is not standard,
        raises error. 
    """

    # perform check on input data and convert to list if needed:
    if isinstance(vars,str):
        vars = [vars]
        
    if isinstance(category,list) and len(category)>1:
        raise ValueError(f'Expect {category} to be a single value, not list')

    # Read in the standardized names
    std_config = _read_std_config()
    df_std_config = _conv_ls_dicts_df_long(std_config )
    # Subset the categories (e.g. target_variables or metrics)
    sub_std_config = df_std_config[df_std_config['category'].str.contains(category)]

    # Check to make sure that each metric is inside the standardized names
    bool_chck = [any(sub_std_config['var'] == x) for x in vars]
    
    if not all(bool_chck):
        bad_vars = list(compress(vars,[not x for x in bool_chck]))
        allowable_vars = ",".join(sub_std_config['var'])
        raise ValueError(f'The following {category} mappings defined in the'
                            ' dataset schema do not correspond to the'
                            f' standardized {category} names:'
                            f' {", ".join(bad_vars)} \n Allowable'
                            f' variables include: {allowable_vars}')
    else:
        print(f'The {category} mappings from the dataset schema match'
                ' expected format.')


def _proc_check_input_df(df: pd.DataFrame, 
                         col_schema_df: pd.DataFrame) -> pd.DataFrame:
    """
    Checks the input dataset for consistency in expected column format as 
        generated from the yaml config file.

    :param df: The dataset of interest containing at a minimum catchment ID 
        and evaluation metrics
    :type df: pd.DataFrame
    :param col_schema_df: The column schema naming convention ingested from 
        the yaml file corresponding to the dataset.
    :type col_schema_df: pd.DataFrame
    :return: wide format df ensuring that the unique identifier for 
        each row is 'gage_id'
    :rtype: pd.DataFrame


    """
    # Changelog/contributions
    #     2024-07-09, originally created, GL
    #     2024-07-11, bugfix in case index is already named 'gage_id', GL

    gage_id = col_schema_df.loc[0, 'gage_id']
    metric_cols = col_schema_df.loc[0, 'metric_cols']
    metrics = metric_cols.split('|')

    # check that all metric columns in schema file are in input dataframe
    # extract column names holding metrics
    metric_columns = metric_cols.split("|")

    if df.columns.isin(metric_columns).sum() != len(metrics):

        # get names of metrics not in df
        missing_columns = [
            col for col in metric_columns if col not in df.columns
            ]

        warnings.warn('\nThe following metric columns are not in your input'
                      f' dataframe, df:\n    {", ".join(missing_columns)}\n'
                      ' \nRevise the config file or ensure'
                       ' the input data is in appropriate format\n'
                        ' (i.e. wide format for each variable)')
 
    if not df.index.name == 'gage_id':
        # Change the name to gage_id   
        df.rename(columns = {gage_id : 'gage_id'},inplace=True)
        if not any(df.columns.str.contains('gage_id')):
            warnings.warn(f'Expecting one df column to be named: {gage_id}'
                          ' - per the config file. Inspect config file'
                          ' and/or dataframe col names')
        # Set gage_id as the index
        if any(df['gage_id'].duplicated()):
            warnings.warn('Expect only one gage_id for each row in the data.'
                          ' Convert df to wide format when passing to'
                          ' proc_col_schema(). This could create problems'
                           ' if writing standardized data in hierarchical format.')
        else: # We can set the index as 'gage_id'
            df.set_index('gage_id', inplace=True)


    # Standardize the metrics
    metric_mappings = col_schema_df['metric_mappings'][0].split('|')

    # Run check that mappings are part of standardized column naming
    _proc_check_std_fs_ids(metric_mappings, category = 'metric')

    # rename metrics to the standardized format
    df = df.rename(columns = dict(zip(metrics, metric_mappings)))

    return df

def proc_col_schema(df: pd.DataFrame, 
                    col_schema_df: pd.DataFrame, 
                    dir_save: str | os.PathLike, 
                    check_nwis: bool = False) -> xr.Dataset:
    """
    Process model evaluation metrics into individual standardized files 
        and save a standardized metadata file.

    :param df: pd.DataFrame type. The dataset of interest containing at a 
        minimum catchment ID and evaluation metrics
    :type df: pd.DataFrame
    :param col_schema_df: The column schema naming convention ingested from 
        the yaml file corresponding to the dataset. To create the schema df,
        refer to :func:`read_schm_ls_of_dict`
    :type col_schema_df: pd.DataFrame
    :param dir_save: Path for saving the standardized metric data file(s)
        and the metadata file.
    :type dir_save: str | os.PathLike
    :param check_nwis: Set to True if NWIS gage ids are the standard location 
        identifier in this dataset. If True, this checks whether NWIS gage ids
        are missing leading zeros and provides a correction if needed. Also
        expects `col_schema_df['featureSource'] == 'nwissite'`.
    :type check_nwis: bool
    :raises ValueError: when dir_save does not contain the expected directory
        structure in cases when saving non-hierarchical file formats
    :return: dataset of the standardized data/metadata
    :rtype: xr.Dataset

    :seealso: :func:`read_schm_ls_of_dict`

    """
    # Changelog/contributions
    #  2024-07-02, originally created, GL


    print(f"Standardizing datasets and writing to {dir_save}")
    # Based on the standardized column schema naming conventions
    dataset_name =  col_schema_df.loc[0, 'dataset_name']
    formulation_id =  col_schema_df.loc[0, 'formulation_id']
    formulation_base =  col_schema_df.loc[0, 'formulation_base']
    save_type = col_schema_df.loc[0, 'save_type']
    save_loc = col_schema_df.loc[0, 'save_loc']

    if formulation_id == None:
        # Create formulation_id as a combination of formulation_base and 
        # other elements
        formulation_id = '_'.join(
            list(
                filter(
                    None,
                    [
                        formulation_base,
                        '_v',
                        col_schema_df.loc[0, 'formulation_ver'], 
                        '_',
                        col_schema_df['dataset_name']
                    ]
                )
            )
        ) 
    
    # Create the unique filename corresponding to a dataset & formulation
    uniq_filename = f'{dataset_name}_{formulation_id}'

    # TODO add cloud or local saving
    if save_loc == 'local':
         # Optionally creates dir structure  if save_type == 'csv' or 'parquet'
        _save_dir_base, _other_save_dirs = _save_dir_struct(
                                                        dir_save, 
                                                        dataset_name, 
                                                        save_type
                                                        )
    elif save_loc == 'aws':
        print("TODO ensure connect credentials here")
        # TODO define _save_dir_base here in case .csv are desired in cloud

    # Run format checker/df renamer on input data based on config file's entries:
    df = _proc_check_input_df(df,col_schema_df)

    # Run format checker on nwissite gage ids for missing leading zeros
    if check_nwis and col_schema_df['featureSource'].values[0] == 'nwissite':
        # Run check on NWIS gage IDS - make sure leading zeros exist where needed.
        df_new = check_fix_nwissite_gageids(
            df=df, 
            gage_id_col = col_schema_df['gage_id'].values[0],
            featureSource = col_schema_df['featureSource'].values[0], 
            featureID=col_schema_df['featureID'].values[0],
            replace_orig_gage_id_col=True)
        
        check_equal_df = df_new.equals(df)
        if not check_equal_df == None:
            warn_str_diff = (f"The {col_schema_df['gage_id'].values[0]} column" 
                          f" in the input dataset has nwissite gage ID values"
                          f"missing leading zeros. Auto-corrected gage ids may not"
                          f" have caught all issues. Consider inspecting input data.")
            warnings.warn(warn_str_diff
                         )
            
            df = df_new.copy()
    elif col_schema_df['featureSource'].values[0] == 'nwissite':
        print(f"The input dataset uses nwissite gage ids. Consider setting\
              \ncheck_nwis=True to run a check on whether the "
              f"{col_schema_df['gage_id'].values[0]} column "
              f"\nin the dataset contains appropriately formatted gage ids, \
              \nspecifically that leading zeros haven't been inadvertently removed.")

    # Convert dataframe to the xarray dataset and add metadata:
    ds = df.to_xarray()
    ds.attrs = col_schema_df.fillna('').to_dict('index')[0]
    
    # TODO query a database for the lat/lon corresponding to the gage-id if 
    # lat/lon not already provided

    # Save the standardized dataset
    if save_type == 'csv' or save_type == 'parquet':
        if len(_other_save_dirs) == 0:
            raise ValueError(
                'Expected _save_dir_struct to generate values in _other_save_dirs'
                )

        # TODO allow output write to a variety of locations (e.g. local/cloud)
        # Write data in long format
        save_path_eval_metr = Path(
            _other_save_dirs['eval_metr'] / f'{uniq_filename}.csv'
            )
             
        if save_type == 'csv':
            df.to_csv(save_path_eval_metr)
        else:
            df.to_parquet(
                Path(str(save_path_eval_metr).replace('.csv','.parquet'))
                )
        # Write metadata table corresponding to these metric data table(s) 
        # (e.g. startDate, endDate)
        save_path_meta = Path(
            _other_save_dirs['meta'] / f'{uniq_filename}_metadata.csv'
            )
        if save_type == 'csv':
            col_schema_df.to_csv(save_path_meta)
        else:
            col_schema_df.to_parquet(
                Path(str(save_path_meta).replace('.csv','.parquet'))
                )
        print(f"Saved files within a sub-directory structure inside {dir_save}")
    elif save_type == 'netcdf':
        save_path_nc = Path(_save_dir_base/Path(f'{uniq_filename}.nc'))
        ds.to_netcdf(save_path_nc)
        print(f"Saved netcdf file as {save_path_nc}")
    elif save_type == 'zarr':
        save_path_zarr = Path(_save_dir_base/Path(f'{uniq_filename}_zarr'))
        if os.path.exists(save_path_zarr):
            shutil.rmtree(save_path_zarr) # Delete any pre-existing zarr data with the same name
                                            # (BChoat-THIS MAY BE RISKY)
        ds.to_zarr(save_path_zarr)   # Re-write to directory
        print(f"Saved zarr files inside {save_path_zarr}")
    return ds # Returning not intended use case, but it's an option


def check_fix_nwissite_gageids(df:pd.DataFrame, gage_id_col:str,
                                featureSource:str = 'nwissite', 
                                featureID:str='USGS-{gage_id}',
                                replace_orig_gage_id_col:bool=True) -> pd.DataFrame:
    """Checks whether USGS gage ID values corresponding to nwissite data follow expected format

    :param df: DataFrame containing a column with nwissite gage id column for format checking
    :type df: pd.DataFrame
    :param gage_id_col: The column name of the gage id column, defaults to 'basin'
    :type gage_id_col: str, optional
    :param featureSource: The :mod:`pynhd` / :language:R: :mod:`nhdplusTools` featureSource describing the source of data, defaults to 'nwissite'
    :type featureSource:  str, optional
    :param featureID: The conversion string to get values inside `df[gage_id_col`] into the `featureSource`'s expected format,, defaults to 'USGS-{gage_id}'
    :type featureID: str, optional
    :param replace_orig_gage_id_col: Should the data inside `df[gage_id_col`] be replaced with the corrected values? If not, an added column named `'fix'` is added, defaults to True
    :type replace_orig_gage_id_col: bool, optional
    :return: The provided `df`, modified in cases when inappropriate `gage_id_col`'s data format found
    :rtype: pd.DataFrame

    """

    ls_still_bad = list()
    if featureSource == 'nwissite':
        print(f"Checking {df.shape[0]} total USGS gage station IDs for appropriate nwissite format.")
        print(f"This may take {round(df.shape[0]/60/3.2,2)} minutes for the first check")
        nldi = nhd.NLDI()
        ls_bad_ids = list()
        for ix, row  in df.iterrows():
            gid = row[gage_id_col]
            try:
                comid = nldi.navigate_byid(fsource=featureSource,fid= featureID.format(gage_id=gid),
                                        navigation='upstreamMain',
                                        source='flowlines',
                                        distance=1 # the shortest distance
                                        ).loc[0]['nhdplus_comid'] 
            except: # Could not process this particular gid
                ls_bad_ids.append(gid)                                                     
        ls_prezero = ['0'+x for x in ls_bad_ids]

        print(f"Checking whether prepending '0' fixes {len(ls_prezero)} total gage_ids that were not recognized during the first check")
        print(f"This may take {round(len(ls_prezero)/60/3.2,2)} minutes for the second check.")
        for prezero in ls_prezero:
            try:
                nldi.navigate_byid(fsource=featureSource,fid= featureID.format(gage_id=prezero),
                                                navigation='upstreamMain',
                                                source='flowlines',
                                                distance=1 # the shortest distance
                                                ).loc[0]['nhdplus_comid']
            except:
                ls_still_bad.append(prezero)
                pass

        
        if len(ls_bad_ids) > 0:
            print('Some improvements to nwissite IDs found')
            conv_df = pd.DataFrame({'wrong_id': ls_bad_ids,
                                    'good_id' : ls_prezero})
            cmbo_df = df.merge(conv_df, left_on = gage_id_col, right_on ='wrong_id', how='left') 
            cmbo_df['fix'] = cmbo_df['good_id']
            cmbo_df.fillna({'fix':cmbo_df[gage_id_col]},inplace=True)
            # In case some values are still bad, set the 'fix' column's bad vals to NA
            cmbo_df.loc[cmbo_df[gage_id_col].isin(ls_still_bad),'fix'] = pd.NA

            
            cmbo_df.drop(columns = ['wrong_id','good_id'], inplace = True)

            if replace_orig_gage_id_col:
                print(f"Replacing original data from the '{gage_id_col}' column with corrected values.")
                cmbo_df[gage_id_col] = cmbo_df['fix']
                cmbo_df.drop(columns = ['fix'],inplace=True )
            else:
                print(f"Corrected values provided in the 'fix' column of the returned DataFrame.")

            df=cmbo_df.copy()                
            if len(ls_still_bad)>0:
                warnings.warn("Some gage_id values still not recognized by USGS nwissite dataset.")
    return df

# Functions to grab catchment attributes using hydrofabric and connected datasets

# Changelog / Contributions
#   2024-07-24 Originally created, GL


library(glue)
library(tidync)
library(dplyr)
library(arrow)
library(nhdplusTools)
library(hydrofabric)
library(hfsubsetR)
library(data.table)
library(pkgcond)

retrieve_attr_exst <- function(comids, vars, dir_db_attrs, bucket_conn=NA){
  #' @title Grab previously-aggregated attributes from locations of interest
  #' @description Retrieves existing attribute data already stored in the
  #' dir_db_attrs directory as .parquet files & return tbl of all comids and
  #' attributes of interest.
  #' @details Only considers data already generated inside dir_db_attrs. If
  #' more data are needed, acquire attribute data acquisition using proc_attr_wrap().
  #' Runs checks on input arguments and retrieved contents, generating warnings
  #' if requested comids and/or variables were completely absent from the dataset
  #' @param comids character class. The comids of interest.
  #' @param vars character class. The attribute variables of interest.
  #' @param dir_db_attrs character class. The path where data
  #' @param bucket_conn Default NA. Placeholder in case a bucket connection is
  #' ever created
  #' @seealso [proc_attr_wrap()]
  #' @export
  # Changelog/Contributions
  #  2024-07-26 Originally created, GL

  # Run checks on input args
  if(!'character' %in% base::class(comids) ){
    # Let's try unlisting and unnaming just-in-case
    comids <- comids %>% base::unlist() %>% base::unname()
    if(!'character' %in% base::class(comids) ){
      warning("comids expected to be character class. converting")
      comids <- base::as.character(comids)
    }
  }
  if(!'character' %in% base::class(vars)){
    # Let's try unlisting and unnaming just-in-case
    vars <- vars %>% base::unlist() %>% base::unname()
    if(!'character' %in% base::class(vars)){
      stop("vars expected to be character class")
    }
  }
  if(!base::dir.exists(dir_db_attrs)){
    stop(glue::glue("The attribute database path does not exist:
                      {dir_db_attrs}"))
  }
  if(!any(base::grepl(".parquet", base::list.files(dir_db_attrs)))){
    warning(glue::glue("The following path does not contain expected
                          .parquet files: {dir_db_attrs}"))
  }

  if(base::is.na(bucket_conn)){
    # Query based on COMID & variables, then retrieve data
    dat_all_attrs <- try(arrow::open_dataset(dir_db_attrs) %>%
                           dplyr::mutate(across(where(is.factor), as.character)) %>% # factors are a pain!!
                           dplyr::filter(featureID %in% !!comids) %>%
                           dplyr::filter(attribute %in% !!vars) %>%
                           dplyr::distinct() %>%
                           dplyr::collect())

    if('try-error' %in% base::class(dat_all_attrs)){
      stop(glue::glue("Could not acquire attribute data from {dir_db_attrs}"))
    }
  } else {# TODO add bucket connection here if it ever becomes a thing
    stop("Need to accommodate a different type of source here, e.g. s3")
  }

  # Run simple checks on retrieved data
  if (base::any(!comids %in% dat_all_attrs$featureID)){
    missing_comids <- comids[base::which(!comids %in% dat_all_attrs$featureID)]
    warning(base::paste0("Datasets missing the following comids: ",
                         base::paste(missing_comids,collapse=","),
                         "\nConsider running proc.attr.hydfab::proc_attr_wrap()"))
  }

  if (base::any(!vars %in% dat_all_attrs$attribute)){
    missing_vars <- vars[base::which(!vars %in% dat_all_attrs$attribute)]
    warning(base::paste0("Datasets entirely missing the following vars: ",
                         base::paste(missing_vars,collapse=","),
                         "\nConsider running proc.attr.hydfab::proc_attr_wrap()"))
  }

  # Run check on all comid-attribute pairings by counting comid-var pairings
  sum_var_df <- dat_all_attrs %>%
    dplyr::group_by(featureID) %>%
    summarise(n_distinct(attribute))
  idxs_miss_vars <- base::which(sum_var_df$`n_distinct(attribute)` != length(vars))
  if(base::length(idxs_miss_vars)>0){
    warning(glue::glue("The following comids are missing desired variables:
              {paste(sum_var_df$featureID[idxs_miss_vars],collapse='\n')}
                       \nConsider running proc.attr.hydfab::proc_attr_wrap()"))
  }

  return(dat_all_attrs)
}


proc_attr_std_hfsub_name <- function(comid,custom_name='', ext='gpkg'){
  #' @title Standardidze hydrofabric subsetter's local filename
  #' @description Internal function that ensures consistent filename
  #' @param comid the USGS common identifier, generated by nhdplusTools
  #' @param custom_name Desired custom name following 'hydrofab_'
  #' @param ext file extension of the hydrofrabric data. Default 'gpkg'

  hfsub_fn <- base::gsub(pattern = paste0(custom_name,"__"),
                         replacement = "_",
                         base::paste0('hydrofab_',custom_name,'_',comid,'.',ext))
  return(hfsub_fn)
}

proc_attr_hydatl <- function(hf_id, s3_path, ha_vars, local_path=NA){
  #' @title Retrieve hydroatlas variables
  #' @description retrieves hydrofabric variables from s3 bucket
  #' @param hf_id numeric. the hydrofabric id, expected to be the COMID
  #' @param s3_path character. full path to the s3 bucket's file holding the hydroatlas data
  #' @param ha_vars list of characters. The variables of interest in the hydroatlas v1
  #' @param local_path character. The local filepath where hydroatlas data are saved to reduce s3 bucket connections.
  #' @export
  # Reads in hydroatlas variables https://data.hydrosheds.org/file/technical-documentation/HydroATLAS_TechDoc_v10_1.pdf

  # if(!is.numeric(hf_id)){
  #   warning(paste0("The hf_id ", hf_id, " expected to be numeric. Converting"))
  #   hf_id <- as.numeric(hf_id)
  # }



  # TODO check for local hydroatlas dataset before proceeding with s3 connection
  if(!base::is.na(local_path)){
    stop(paste0("The following path does not exist for saving hydroatlas
                   data:\n",local_path))

  } else {
    bucket <- try(arrow::s3_bucket(s3_path))
    if('try-error' %in% base::class(bucket)){
      stop(glue::glue("Could not connect to an s3 bucket path for hydroatlas
                      data retrieval. Reconsider the s3_path of {s3_path}"))
    }

    ha <- arrow::open_dataset(s3_path) %>%
      dplyr::filter(hf_id %in% !!hf_id) %>%
      dplyr::select("hf_id", any_of(ha_vars)) %>%
      dplyr::collect()
  }

  if(!base::is.na(local_path)){
    # TODO generate standard hydroatlas filename

    # TODO write hydroatlas filename
  }
  return(ha)
}

proc_attr_usgs_nhd <- function(comid,usgs_vars){
  #' @title Retrieve USGS variables based on comid
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param usgs_vars list class. The standardized names of NHDplus variables.
  #' @seealso \code{nhdplusTools::get_characteristics_metadata() }
  #' @export
  # Get the s3 urls for each variable of interest
  usgs_meta<- nhdplusTools::get_characteristics_metadata() %>%
    dplyr::filter(ID %in% usgs_vars)
  # Extract the variable data corresponding to the COMID
  ls_usgs_mlti <- list()
  for (r in 1:nrow(usgs_meta)){
    var_id <- usgs_meta$ID[r]
    ls_usgs_mlti[[r]] <- arrow::open_dataset(usgs_meta$s3_url[r]) %>%
      dplyr::select(dplyr::all_of(c("COMID",var_id))) %>%
      dplyr::collect() %>%
      dplyr::filter(COMID==!!comid) #%>%
  }
  # Combining it all
  usgs_subvars <- ls_usgs_mlti %>% purrr::reduce(dplyr::full_join, by = 'COMID')
  return(usgs_subvars)
}


proc_attr_hf <- function(comid, dir_db_hydfab,custom_name="{lyrs}_",ext = 'gpkg',
                         lyrs=c('divides','network')[2],
                         hf_cat_sel=TRUE, overwrite=FALSE){

  #' @title Retrieve hydrofabric data of interest based on location identifier
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Checks to see if a local dataset exists. If not, retrieve from lynker-spatial s3 bucket
  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param dir_db_hydfab character class. Local directory path for storing hydrofabric data
  #' @param custom_name character class. A custom name to insert into hydrofabric file. Default \code{glue("{lyrs}_")}
  #' @param ext character class. file extension of hydrofabric file. Default 'gpkg'
  #' @param lyrs character class. The layer name(s) of interest from hydrofabric. Default 'network'.
  #' @param hf_cat_sel boolean. TRUE for a total catchment characterization specific to a single comid, FALSE (or anything else) for all subcatchments
  #' @param overwrite boolean. Overwrite local data when pulling from hydrofabric s3 bucket? Default FALSE.
  #' @export

  # Build the hydfab filepath
  name_file <- proc.attr.hydfab:::proc_attr_std_hfsub_name(comid=comid,
                                   custom_name=glue::glue('{lyrs}_'),
                                   ext=ext)
  fp_cat <- base::file.path(dir_db_hydfab, name_file)

  if(!base::dir.exists(dir_db_hydfab)){
    warning(glue::glue("creating the following directory: {dir_db_hydfab}"))
    base::dir.create(dir_db_hydfab)
  }

  # Generate the nldi feature listing
  nldi_feat <- list(featureSource ="comid",
                         featureID = comid)

  # Download hydrofabric file if it doesn't exist already
  # Utilize hydrofabric subsetter for the catchment and download to local path
  pkgcond::suppress_warnings(hfsubsetR::get_subset(nldi_feature = nldi_feat,
                        outfile = fp_cat,
                        type = 'reference',lyrs = lyrs,
                        overwrite=overwrite),pattern="exists and overwrite is FALSE")

  # Read the hydrofabric file gpkg for each layer
  hfab_ls <- list()
  if (ext == 'gpkg') {
    # Define layers
    layers <- sf::st_layers(dsn = fp_cat)
    for (lyr in layers$name){
      hfab_ls[[lyr]] <- sf::read_sf(fp_cat,layer=lyr)
    }
  } else {
    stop("# TODO add in the type of hydrofabric file to read based on extension")
  }
  net <- hfab_ls[[lyrs]] %>%
    dplyr::select(divide_id, hf_id) %>%
    dplyr::filter(complete.cases(.)) %>%
    dplyr::group_by(divide_id) %>% dplyr::slice(1)

  if (hf_cat_sel==TRUE){
    # interested in the single location's aggregated catchment data
    net <- net %>% base::subset(hf_id==base::as.numeric(comid))
  }
  return(net)
}

proc_attr_exst_wrap <- function(comid,path_attrs,vars_ls,bucket_conn=NA){
  #' @title Existing attribute data checker
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Retrieves what attribute data already exists in a data storage
  #'  path for a given comid and identifies missing attributes.
  #'  Returns list of
  #'   - dt_all: a data.table of existing comid data,
  #'   - need_vars: a list of datasource ids containing a list of variable
  #'        names that will be downloaded.

  #' @param comid character class. The common identifier USGS location code for a surface water feature.
  #' @param path_attrs character. Path to attribute file data storage location
  #' @param vars_ls list. Variable names
  #' @param bucket_conn TODO add cloud conn details in case data stored in s3
  #' @seealso [proc_attr_wrap()]
  #' @export
  #'
  # Changelog / Contributions
  #  2024-07-25 Originally created, GL

  # TODO adapt this check if stored in cloud (e.g. s3 connection checker)
  # Check that data has been created
  path_attrs_exst <- any(c(base::file.exists(path_attrs)))

  # Also make sure the directory exists:
  if(!dir.exists(base::dirname(path_attrs)) && is.na(bucket_conn)){
    dir.create(base::dirname(path_attrs))
  } # TODO adapt if stored in cloud (e.g. s3 connection checker)

  if(path_attrs_exst==TRUE){
    dt_all <- arrow::open_dataset(path_attrs) %>% data.table::as.data.table()
    need_vars <- list()
    for(var_srce in names(vars_ls)){
      # Compare/contrast what is there vs. desired
      attrs_reqd <- vars_ls[[var_srce]]
      attrs_needed <- attrs_reqd[which(!attrs_reqd %in% dt_all$attribute)]

      if(length(attrs_needed)>0){ # Only build list of variables needed
        need_vars[[var_srce]] <- attrs_needed
      }
    }
  } else {
    # No variable subsetting required. Grab them all for this comid
    need_vars <- vars_ls
    dt_all <- data.table::data.table() # to be populated.
  }
  return(list(dt_all=dt_all,need_vars=need_vars))
}

proc_attr_wrap <- function(comid, Retr_Params, lyrs='network',overwrite=FALSE){
  #' @title Wrapper to retrieve variables when processing attributes
  #' @author Guy Litt \email{guy.litt@noaa.gov}
  #' @description Identifies a comid location using the hydrofabric and then
  #' acquires user-requested variables from multiple sources. Writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Re-processing runs only download data that have not yet been acquired.
  #' @details Function returns & writes a data.table of all these fields:
  #'   featureID - e.g. USGS common identifier (default)
  #'   featureSource - e.g. "COMID" (default)
  #'   data_source - where the data came from (e.g. 'usgs_nhdplus__v2','hydroatlas__v1')
  #'   dl_timestamp - timestamp of when data were downloaded
  #'   attribute - the variable identifier used in a particular dataset
  #'   value - the value of the identifier
  #' @param comid character. The common identifier USGS location code for a surface water feature.
  #' @param Retr_Params list. List of list structure with parameters/paths needed to acquire variables of interest
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @seealso \code{\link{proc_attrs_gageids}}
  #' @export

  # Changelog / Contributions
  #   2024-07-25 Originally created, GL
  message(base::paste0("Processing COMID ",comid))

  # Retrieve the hydrofabric id
  net <- try(proc.attr.hydfab::proc_attr_hf(comid=comid,
                                        dir_db_hydfab=Retr_Params$paths$dir_db_hydfab,
                                        custom_name ="{lyrs}_",
                                        lyrs=lyrs,overwrite=overwrite))
  if ('try-error' %in% base::class(net)){
    warning(glue::glue("Could not acquire hydrofabric for comid {comid}. Proceeding to acquire variables of interest without hydrofabric."))
    net <- list()
    net$hf_id <- comid
  }

  path_attrs <- base::file.path(Retr_Params$paths$dir_db_attrs,
                          base::paste0("comid_",comid,"_attrs.parquet"))
  vars_ls <- Retr_Params$vars
  # ----------- existing dataset checker ----------- #
  ls_chck <- proc.attr.hydfab::proc_attr_exst_wrap(comid,path_attrs,
                                                   vars_ls,bucket_conn=NA)
  dt_all <- ls_chck$dt_all
  need_vars <- ls_chck$need_vars

  # --------------- dataset grabber ---------------- #
  attr_data <- list()
  if (('ha_vars' %in% base::names(need_vars)) &&
      (base::all(!base::is.na(need_vars$ha_vars)))){
    # Hydroatlas variable query; list name formatted as {dataset_name}__v{version_number}
    attr_data[['hydroatlas__v1']] <- proc.attr.hydfab::proc_attr_hydatl(s3_path=Retr_Params$paths$s3_path_hydatl,
                                          hf_id=net$hf_id,
                                          ha_vars=need_vars$ha_vars) %>%
                                # ensures 'COMID' exists as colname
                                dplyr::rename("COMID" = "hf_id")
  }
  if( (base::any(base::grepl("usgs_vars", base::names(need_vars)))) &&
      (base::all(!base::is.na(need_vars$usgs_vars))) ){
    # USGS nhdplusv2 query; list name formatted as {dataset_name}__v{version_number}
    attr_data[['usgs_nhdplus__v2']] <- proc.attr.hydfab::proc_attr_usgs_nhd(comid=net$hf_id,
                                                                usgs_vars=need_vars$usgs_vars)
  }
  ########## May add more data sources here and append to attr_data ###########
  # ----------- dataset standardization ------------ #
  if (!base::all(base::unlist(( # A qa/qc check
          base::lapply(attr_data, function(x)
                  base::any(base::grepl("COMID", colnames(x)))))))){
    stop("Expecting 'COMID' as a column name identifier in every dataset")
  }
  # Ensure consistent format of dataset
  attr_data_ls <- list()
  for(dat_srce in base::names(attr_data)){
    sub_dt_dat <- attr_data[[dat_srce]] %>% data.table::as.data.table()
    # Even though COMID always expected, use featureSource and featureID for
    #.  full compatibility with potential custom datasets
    sub_dt_dat$featureID <- base::as.character(sub_dt_dat$COMID)
    sub_dt_dat$featureSource <- "COMID"
    sub_dt_dat$data_source <- base::as.character(dat_srce)
    sub_dt_dat$dl_timestamp <- base::as.character(base::as.POSIXct(
      base::format(Sys.time()),tz="UTC"))
    sub_dt_dat <- sub_dt_dat %>% dplyr::select(-COMID)
    # Convert from wide to long format
    attr_data_ls[[dat_srce]] <- data.table::melt(sub_dt_dat,
                             id.vars = c('featureID','featureSource', 'data_source','dl_timestamp'),
                             variable.name = 'attribute')
  }
  # Combine freshly-acquired data
  dt_new_dat <- data.table::rbindlist(attr_data_ls)

  # Combined dt of existing data and newly acquired data
  if(base::dim(dt_all)[1]>0 && base::dim(dt_new_dat)[1]>0){
    dt_cmbo <- data.table::merge.data.table(dt_all,dt_new_dat,
                                            all=TRUE,no.dups=TRUE)
  } else if (base::dim(dt_new_dat)[1] >0){
    dt_cmbo <- dt_new_dat
  } else {
    dt_cmbo <- dt_all
  }
  # Remove all factors to make arrow::open_dataset() easier to work with
  dt_cmbo <- dt_cmbo %>% dplyr::mutate(across(where(is.factor), as.character))

  # Write attribute variable data specific to a comid here
  arrow::write_parquet(dt_cmbo,path_attrs)
  return(dt_cmbo)
}

proc_attr_gageids <- function(gage_ids,featureSource,featureID,Retr_Params,
                              lyrs="network",overwrite=FALSE){
  #' @title Process catchment attributes based on vector of gage ids.
  #' @description
  #' Prepares inputs for main processing step. Iterates over each location
  #' for grabbing catchment attribute data corresponding to the gage_id
  #' location. Acquires user-requested variables from multiple catchment
  #' attribute sources. Calls \code{\link{proc_attr_wrap}} which writes all
  #' acquired variables to a parquet file as a standard data.table format.
  #' Returns a list of comids that corresponded to the gage_ids
  #' @param gage_ids array of gage_id values to be queried for catchment attributes
  #' @param featureSource The \code{\link[nhdplusTools]{get_nldi_features}}feature featureSource,
  #' e.g. 'nwissite'
  #' @param featureID a glue-configured conversion of gage_id into a recognized
  #' featureID for \code{\link[nhdplusTools]{get_nldi_features}}. E.g. if gage_id
  #' represents exactly what the nldi_feature$featureID should be, then
  #'  featureID="{gage_id}". In other instances, conversions may be necessary,
  #'  e.g. featureID="USGS-{gage_id}". When defining featureID, it's expected
  #'  that the term 'gage_id' is used as a variable in glue syntax to create featureID
  #' @param Retr_Params list. List of list structure with parameters/paths
  #' needed to acquire variables of interest. List objects include the following:
  #'  \itemize{
  #'  \item \code{paths} list of directories or paths used to acquire and save data These include the following:
  #'  \item \code{paths$dir_db_hydfab} the local path to where hydrofabric data are saved
  #'  \item \code{path$dir_db_attrs} local path for saving catchment attributes as parquet files
  #'  \item \code{path$s3_path_hydatl} the s3 location where hydroatlas data exist
  #'  \item \code{path$dir_std_base} the location of user_data_std containing dataset that were standardized by \pkg{fsds_proc}.
  #'  \item \code{datasets} character vector. A list of datasets of interest inside \code{paths$dir_std_base}. Not used in \code{proc_attr_gageids}
  #'  }
  #' @param lyrs character. The layer names of interest from the hydrofabric gpkg. Default 'network'
  #' @param overwrite boolean. Should the hydrofabric cloud data acquisition be redone and overwrite any local files? Default FALSE.
  #' @export
  #  Changelog/Contributions
  #   2024-07-29 Originally created, GL

  # Path checker/maker of anything that's a directory
  for(dir in Retr_Params$paths){
    if(base::grepl('dir',dir)){
      if(!base::dir.exists(dir)){
        message(glue::glue("Creating {dir}"))
        base::dir.create(dir)
      }
    }
  }

  ls_comid <- base::list()
  for (gage_id in gage_ids){ #
    if(!base::exists("gage_id")){
      stop("MUST use 'gage_id' as the object name!!! \n
        Expected when defining nldi_feat$featureID")
    }

    # Retrieve the COMID
    # Reference: https://doi-usgs.github.io/nhdplusTools/articles/get_data_overview.html
    nldi_feat <- base::list(featureSource =featureSource,
                            featureID = as.character(glue::glue(featureID)) # This should expect {'gage_id'} as a variable!
    )
    site_feature <- try(nhdplusTools::get_nldi_feature(nldi_feature = nldi_feat))
    if('try-error' %in% class(site_feature)){
      stop(glue::glue("The following nldi features didn't work. You may need to
             revisit the configuration yaml file that processes this dataset in
            fsds_proc: \n {featureSource}, and featureID={featureID}"))
    } else if (!is.null(site_feature)){
      comid <- site_feature['comid']$comid
      ls_comid[[gage_id]] <- comid
      # Retrieve the variables corresponding to datasets of interest & update database
      loc_attrs <- try(proc.attr.hydfab::proc_attr_wrap(comid=comid,
                                                    Retr_Params=Retr_Params,
                                                    lyrs='network',overwrite=FALSE))
      if("try-error" %in% class(loc_attrs)){
        message(glue::glue("Skipping gage_id {gage_id} corresponding to comid {comid}"))
      }
    } else {
      message(glue::glue("Skipping {gage_id}"))
    }
  }
  just_comids <- ls_comid %>% unname() %>% unlist()

  if(any(is.na(just_comids))){
    idxs_na_comids <- which(is.na(just_comids))
    gage_ids_missing <- paste0(names(ls_comid[idxs_na_comids]), collapse = ", ")
    warning(glue::glue("The following gage_id values did not return a comid:\n
                       {gage_ids_missing}"))
  }
  return(ls_comid)
}

read_loc_data <- function(loc_id_filepath, loc_id, fmt = 'csv'){
  #' @title Read location identifiers
  #' @description Reads directly from a csv or arrow-compatible dataset.
  #' Returns the dataset's column identifer renamed as 'gage_id' in a tibble
  #' @param loc_id_filepath csv filepath or dataset filepath/directory.
  #' @param loc_id The column name of the identifier column
  #' @param fmt The format passed to arrow::open_dataset() in the non-csv case.
  #' Default 'csv'. May also be 'parquet', 'arrow', 'feather', 'zarr', etc.
  #' @seealso [proc_attr_read_gage_ids_fsds()]
  #' @seealso [proc_attr_wrap()]
  #' @export
  # Changelog / contributions
  #  2024-08-09 Originally created

  if (!base::is.null(loc_id_filepath)){
    # Figure out the colnames of everything in the dataset.
    cols <- arrow::open_dataset(loc_id_filepath, format = fmt) %>% base::colnames()
    # assign every col as a character string because leading zeros risk being dropped
    schema <- arrow::schema(!!!setNames(rep(list(arrow::string()), length(cols)), cols))
    # Read in dataset
    if (grepl('tsv|text|csv|txt',tools::file_ext(loc_id_filepath))){
      dat_loc <- arrow::open_dataset(loc_id_filepath,format = fmt,
                                     col_types=schema) %>%
        dplyr::select(dplyr::all_of(loc_id)) %>% dplyr::collect() %>%
        dplyr::rename('gage_id' = loc_id)
    } else {
      dat_loc <- arrow::open_dataset(loc_id_filepath,format = fmt,
                                     schema=schema) %>%
        dplyr::select(dplyr::all_of(loc_id)) %>% dplyr::collect() %>%
        dplyr::rename('gage_id' = loc_id)
    }

  } else {
    base::message(glue::glue("No location dataset defined. Reconsider designation for \n {loc_id_filepath}."))
    dat_loc <- NULL
  }
  return(dat_loc)
}

proc_attr_read_gage_ids_fsds <- function(dir_dataset, ds_filenames=''){
  #' @title Read in standardized FSDS gage_id location identifiers
  #' @description Reads output generated using \pkg{fsds_proc} python package and
  #' selects the gage_id location identifier(s) and the
  #' featureSource & featureID that correspond to the gage_id
  #' @param dir_dataset directory path to the dataset
  #' @param ds_filenames a matching string specific to dataset(s) of interest
  #' inside \code{dir_dataset}
  #' @details Returns a list of the following objects:
  #' gage_ids: array of gage_id values
  #' featureSource: The type of nhdplus feature source corresponding to gage_id
  #' featureID: The method of converting gage_id into a standardized featureSource's featureID
  #' @seealso \code{\link[nhdplusTools]{get_nldi_features}}
  #' @export

  # Changelog/contributions
  #  2024-07-29 Originally created, GL

  # ----  Read in a standard format filename and file type from fsds_proc ---- #
  dir_ds <- base::file.path(dir_dataset)
  files_ds <- base::list.files(dir_ds)
  fns <- base::lapply(ds_filenames,
                      function(x) files_ds[base::grep(x,files_ds)]) %>% unlist()

  if (base::any(base::grepl(".nc",fns))){ # Read in a netcdf file
    fn_nc <- fns[base::grep(".nc",fns)]
    if(length(fn_nc)!=1){
      stop(glue::glue("Expected that only one netcdf file exists in dir:\n{dir_ds}"))
    }
    dat_in <- file.path(dir_dataset,fn_nc)
    nc <- ncdf4::nc_open(dat_in)

    # Grab the gage_id identifier:
    gage_ids <- nc$dim$gage_id$vals

    # Extract attributes of interest that describe what gage_id represents
    attrs <- ncdf4::ncatt_get(nc,varid=0)
    featureSource <- attrs$featureSource
    featureID <- attrs$featureID # intended to reformat gage_id into the appropriate nldi format using glue(e.g. glue('USGS-{gage_id}')
  } else {
    print(paste0("The following contents inside \n",dir_ds,
                 "\n do not match expected format:\n", paste0(fns, collapse = ", ")))
    stop("Create a different file format reader here that generates everything in the return list.")
    # TODO make this more adaptable so that it doesn't depend on running python fsds_proc beforehand
    # Idea: e.g. read in user-defined gage_id data as a .csv
    # Idea: read in gage_id data inside a non-standard netcdf file, then define featureSource and featureID from a separate yaml file
  }
  return(base::list(gage_ids=gage_ids, featureSource=featureSource, featureID=featureID))
}

grab_attrs_datasets_fsds_wrap <- function(Retr_Params,lyrs="network",overwrite=FALSE){
  #' @title Grab catchment attributes from processed FSDS input
  #' @description Wrapper function that acquires catchment attribute data from
  #' FSDS processed input generated via \pkg{fsds_proc} package
  #' @param Retr_Params list of parameters built for grabbing catchment attribute data. List objects include the following:
  #'  \itemize{
  #'  \item \code{paths} list of directories or paths used to acquire and save data These include the following:
  #'  \item \code{paths$dir_db_hydfab} the local path to where hydrofabric data are saved
  #'  \item \code{path$dir_db_attrs} local path for saving catchment attributes as parquet files
  #'  \item \code{path$s3_path_hydatl} the s3 location where hydroatlas data exist
  #'  \item \code{path$dir_std_base} the location of user_data_std containing dataset that were standardized by \pkg{fsds_proc}.
  #'  \item \code{datasets} character vector. A list of datasets of interest inside \code{paths$dir_std_base}. If 'all' is specified, then all datasets in the directory are processed.
  #'  }
  #' @param overwrite boolean default FALSE. Should hydrofabric data be overwritten?
  #' @param lyrs default "network" the hydrofabric layers of interest.
  #'  Only 'network' is needed for attribute grabbing.
  #' @details Runs two proc.attr.hydfab functions:
  #'  \code{\link{proc_attr_read_gage_ids_fsds}} - retrieves the gage_ids generated by \pkg{fsds_proc}
  #'  \code{\link{proc_attr_gageids}} - retrieves the attributes for all provided gage_ids
  #'
  #' @export
  # Changelog/contributions
  #  2024-07-30 Originally created, GL

  # 'all' an option if processing all datasets desired. Otherwise list datasets in config file
  all_ds <- base::basename(base::list.dirs(Retr_Params$paths$dir_std_base,recursive=F))
  if(base::is.null(Retr_Params$datasets)){
    datasets <- NULL
  } else if (Retr_Params$datasets[1]=='all'){ # Process all datasets inside a directory
    datasets <- all_ds
  } else { # Only process those datasets listed inside the directory
    datasets <- Retr_Params$datasets
  }

  if(base::any(!datasets %in% all_ds)){ # Run check that dataset exists
    bad_ds <- paste0(datasets[which(!datasets %in% all_ds)], collapse = ", ")
    good_ds <- paste0(all_ds, collapse = ", ")
    stop(base::paste0("The following datasets do not exist in the directory\n",
                      Retr_Params$paths$dir_std_base, "/: \n ",bad_ds,"\n",
                      "\n These options exist in that directory:\n",good_ds,
                      "\n\n Reconsider the dataset and/or directory choice."))
  }

  ls_comids_all <- base::list()
  for(dataset_name in datasets){ # Looping by dataset
    message(glue::glue("--- PROCESSING {dataset_name} DATASET ---"))
    dir_dataset <- base::file.path(Retr_Params$paths$dir_std_base,dataset_name)

    # Retrieve the gage_ids, featureSource, & featureID from fsds_proc standardized output
    ls_fsds_std <- proc.attr.hydfab::proc_attr_read_gage_ids_fsds(dir_dataset)
    # TODO add option to read in gage ids from a separate data source
    gage_ids <- ls_fsds_std$gage_ids
    featureSource <- ls_fsds_std$featureSource
    featureID <- ls_fsds_std$featureID

    # ---------------------- Grab all needed attributes ---------------------- #
    ls_comids <- proc.attr.hydfab::proc_attr_gageids(gage_ids,
                                                     featureSource,
                                                     featureID,
                                                     Retr_Params,
                                                     lyrs=lyrs,
                                                     overwrite=overwrite)
    ls_comids_all[[dataset_name]] <- ls_comids
  }
  # -------------------------------------------------------------------------- #
  # ------------ Grab attributes from a separate loc_id file ----------------- #
  if (!base::is.null(Retr_Params$loc_id_read$loc_id_filepath)){
    # Generate list of identifiers
    dat_loc <- proc.attr.hydfab::read_loc_data(Retr_Params$loc_id_read$loc_id_filepath,
                                               Retr_Params$loc_id_read$gage_id,
                                               fmt = Retr_Params$loc_id_read$fmt)

    if(base::nrow(dat_loc)>0){
      # TODO bugfix this here
      loc_id <- Retr_Params$loc_id_read$loc_id
      ls_comids_loc <- proc.attr.hydfab::proc_attr_gageids(gage_ids=as.array(dat_loc[['gage_id']]),
                                                           featureSource=Retr_Params$loc_id_read$featureSource_loc,
                                                           featureID=Retr_Params$loc_id_read$featureID_loc,
                                                           Retr_Params,
                                                           lyrs=lyrs,
                                                           overwrite=overwrite)
    } else {
      # TODO add check that user didn't provide parameter expecting to read data
    }
    # Combine lists
    ls_comids_all[[Retr_Params$loc_id_read$loc_id_filepath]] <- ls_comids_loc
  }



  return(ls_comids_all)
}

# Config for grabbing catchment attributes corresponding to standard-named locations
# Two options exist for defining locations that need attributes. At least one must be used. Both may be used.
# 1. Refer to a file/dataset {loc_id_filepath} with a column identifer {loc_id} representing a standardized location identifier.
# 2. Refer to a dataset processed by fsds_proc python package and point to its location, {dir_std_base}/{datasets}, where {datasets} is a specific subdirectory name(s) or simply 'all'

col_schema:   # required column mappings in the evaluation metrics dataset (if read in)
  - 'featureID':  # python f-string / R glue() format; converting the 'gage_id' to the standardized featureID used by nhdplusTools/hydrofabric. Must use '{gage_id}' e.g. 'USGS-{gage_id}'
  - 'featureSource':  # The standardized nhdplusTools featureSource. Possible featureSources might be 'nwissite', 'comid'.
loc_id_read: # This section only required for locations NOT to be read in under a standardized dataset location (dir_std_base). May be used for additional prediction locations. MUST leave each item name inside list with empty assignments if no datasets desired.
  - 'gage_id': 'gage_id' # expects tabular dataset with this column name representing the location id. Must be 1st item in this sublist.
  - 'loc_id_filepath': '{dir_std_base}/juliemai-xSSA/eval/metrics/juliemai-xSSA_Raven_blended.csv' #Filepath. Allows reading of .csv or a dataset accessible using arrow::open_datast(). Must be 2nd item in this sublist.
  - 'featureID_loc' : 'USGS-{gage_id}' # python f-string / R glue() format; converting the 'loc_id' to the standardized featureID used by nhdplusTools/hydrofabric. Must use '{loc_id}' e.g. 'USGS-{loc_id}'. Must be 3rd item in this sublist.
  - 'featureSource_loc': 'nwissite' # The standardized nhdplusTools featureSource. Must be 4th item in this sublist.
file_io: # May define {home_dir} for python's '{home_dir}/string_path'.format(home_dir =str(Path.home())) functionality
  - 'dir_save': '{home_dir}/noaa/regionalization/data/input' #TODO determine if this needed.
  - 'save_type':  #  Required. Use 'csv' to create a directory structure & save multiple files. May also save as hierarchical files 'netcdf' or 'zarr', or if 'csv' chosen, a directory structure is created
  - 'save_loc': 'local' #  Required.  Use 'local' for saving to a local path via dir_save. Future work will create an approach for 'aws' or other cloud saving methods
  - 'dir_base' : '{home_dir}/noaa/regionalization/data/input' # Required. The save location of standardized output
  - 'dir_std_base' : '{dir_base}/user_data_std' # Required. The location of standardized data generated by proc_fsds python package
  - 'dir_db_hydfab' : '{dir_base}/hydrofabric' # Required. The local dir where hydrofabric data are stored (limits the total s3 connections)
  - 'dir_db_attrs' : '{dir_base}/attributes' # Required. The parent dir where each comid's attribute parquet file is stored in the subdirectory 'comid/', and each dataset's aggregated parquet attributes are stored in the subdirectory '/{dataset_name}
formulation_metadata:  
  - 'datasets': # Required. Must match directory name inside dir_std_base. May be a list of items, or simply sublist 'all' to select everything inside dir_std_base for attribute grabbing.
    -  # Required. In this example case, it's a sublist of just one thing.
  - 'formulation_base': # Informational. Unique name of formulation.
hydfab_config: # Required section describing hydrofabric connection details and objects of interest
 - 's3_base' : "s3://lynker-spatial/tabular-resources" # Required. s3 path containing hydrofabric-formatted attribute datasets
 - 's3_bucket' : 'lynker-spatial' # Required. s3 bucket containing hydrofabric data
 - 'ext' : 'gpkg' # Required. file extension of the hydrofrabric data. Default 'gpkg'. 
 - 'hf_cat_sel': "total" # Required. Options include 'total' or 'all'; total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
attr_select: # The names of variable sublistings are standardized, e.g. ha_vars, usgs_vars, sc_vars
  - 's3_path_hydatl' : '{s3_base}/hydroATLAS/hydroatlas_vars.parquet' # path to hydroatlas data formatted for hydrofabric. Required only if hydroatlas variables desired.
  - 'ha_vars':  # hydroatlas variables. Must specify s3_path_hydatl if desired.
    - 'pet_mm_s01'
    - 'cly_pc_sav'
    - 'cly_pc_uav'
  - 'usgs_vars': # list of variables retrievable using nhdplusTools::get_characteristics_metadata(). 
    - 'TOT_TWI'
    - 'TOT_PRSNOW'
    - 'TOT_POPDENS90'
    - 'TOT_EWT'
    - 'TOT_RECHG'
  - 'sc_vars': # Streamcat variables of interest. #TODO add streamcat grabber capability to proc.attr.hydfab
    - # In this example case, no streamcat variables selected
references: # All optional but **very** helpful metadata
  - 'input_filepath': '{base_dir}/julemai-xSSA/data_in/basin_metadata/basin_validation_results.txt'
  - 'source_url': 'https://zenodo.org/records/5730428'
  - 'dataset_doi': '10.5281/zenodo.5730428'
  - 'literature_doi': 'https://doi.org/10.1038/s41467-022-28010-7'

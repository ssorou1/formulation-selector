# Config for grabbing catchment attributes corresponding to standard-named locations
# Two options exist for defining locations that need attributes. At least one must be used. Both may be used.
# 1. Refer to a file/dataset {loc_id_filepath} with a column identifer {loc_id} representing a standardized location identifier.
# 2. Refer to a dataset processed by fs_proc python package and point to its location, {dir_std_base}/{datasets}, where {datasets} is a specific subdirectory name(s) or simply 'all'

col_schema:   # required column mappings in the evaluation metrics dataset (if read in)
  - featureID: 'USGS-{gage_id}' # python f-string / R glue() format; converting the 'gage_id' to the standardized featureID used by nhdplusTools/hydrofabric. Must use '{gage_id}' e.g. 'USGS-{gage_id}'
  - featureSource: 'nwissite' # The standardized nhdplusTools featureSource. Possible featureSources might be 'nwissite', 'comid'.
loc_id_read: # This section only required for locations NOT to be read in under a standardized dataset location (dir_std_base). May be used for additional prediction locations. MUST leave each item name inside list with empty assignments if no datasets desired.
  - gage_id: 'gage_id' # expects tabular dataset with this column name representing the location id.
  - loc_id_filepath: '' # Required. filepath. Allows reading of .csv or a dataset accessible using arrow::open_datast() in lieu of reading dataset generated by fs_proc.
  - featureID_loc: 'USGS-{gage_id}' # python f-string / R glue() format; converting the 'loc_id' to the standardized featureID used by nhdplusTools/hydrofabric. Must use '{loc_id}' e.g. 'USGS-{loc_id}'.
  - featureSource_loc: 'nwissite' # The standardized nhdplusTools featureSource.
file_io: # May define {home_dir} for python's '{home_dir}/string_path'.format(home_dir =str(Path.home())) functionality
  - save_loc: 'local' #  #TODO implement once s3 becomes a capability. Use 'local' for saving to a local path via dir_save. Future work will create an approach for 'aws' or other cloud saving methods
  - dir_base: '{home_dir}/noaa/regionalization/data/input' # Required. The save location of standardized output
  - dir_std_base: '{dir_base}/user_data_std' # Required. The location of standardized data generated by fs_proc python package
  - dir_db_hydfab: '{dir_base}/hydrofabric' # Required. The local dir where hydrofabric data are stored (limits the total s3 connections)
  - dir_db_attrs: '{dir_base}/attributes' # Required. The parent dir where each comid's attribute parquet file is stored in the subdirectory 'comid/', and each dataset's aggregated parquet attributes are stored in the subdirectory '/{dataset_name}
  - ds_type: 'training' # Required string. Recommended to select 'training' or 'prediction', but any string will work. This string will be used in the filename of the output metadata describing each data point's identifer, COMID, lat/lon, reach name of the location. This string should differ from the string used in the prediction config yaml file. Filename: `"nldi_feat_{dataset}_{ds_type}.csv"` inside `dir_std_base / dataset / `
  - write_type: 'parquet' # Required filetype for writing NLDI feature metadata. Default 'parquet'. May also select 'csv'
  - path_meta:  "{dir_std_base}/{ds}/nldi_feat_{ds}_{ds_type}.{write_type}"  #Required. Training attribute metadata filepath formatted for R's glue or py f-string, as generated using `proc.attr.hydfab::write_meta_nldi_feat()`. Strongly suggested default:  "{dir_std_base}/{ds}/nldi_feat_{ds}_{ds_type}.{write_type}"  
formulation_metadata:
  - datasets: # Required. Must match directory name inside dir_std_base. May be a list of items, or simply sublist 'all' to select everything inside dir_std_base for attribute grabbing.
    - 'juliemai-xSSA' # Required. In this example case, it's a sublist of just one thing.
  - formulation_base: 'Raven_blended' # Informational. Unique name of formulation. Optional.
hydfab_config: # Required section describing hydrofabric connection details and objects of interest, particularly for hfsubsetR::get_subset()
 - s3_base: "s3://lynker-spatial/tabular-resources" # Required. s3 path containing hydrofabric-formatted attribute datasets
 - s3_bucket: 'lynker-spatial' # Required. s3 bucket containing hydrofabric data
 - hf_cat_sel: "total" # Required. Options include 'total' or 'all'; total: interested in the single location's aggregated catchment data; all: all subcatchments of interest
 - ext: 'gpkg' # The file extension
 - gpkg:  # Optional. A local gpkg file. Default 'NULL'. See hfsubsetR::get_subset()
 - hfab_retr: FALSE # Optional, Boolean. Defaults to the hfab_retr argument default in the proc_attr_wrap() function (TRUE). Should the hydrofabric data be downloaded? Hydrofabric data download may not be necessary. Processing is faster if set to FALSE
 - hf_version: "2.1.1"  # Optional, character string. Defaults to the hf_version argument default in hfsubsetR::get_subset() function. The hydrofabric version.
 - domain: "conus"  # Optional, character string. Defaults to the hf_version argument default in hfsubsetR::get_subset() function. Ignored if hfab_retr = FALSE. The hydrofabric domain.
 - type: "nextgen"  # Optional, character string. Defaults to the hf_version argument default in hfsubsetR::get_subset() function. Ignored if hfab_retr = FALSE. The hydrofabric type.
 - lyrs: # Optional, sublist of character strings. Defaults to the hf_version argument default in hfsubsetR::get_subset() function. Ignored if hfab_retr = FALSE. Hydrofabric layers to extract.
   - 'divides'
   - 'network'
 - source: "s3://lynker-spatial/hydrofabric"
attr_select: # Required. The names of variable sublistings are standardized with _vars, e.g. ha_vars, usgs_vars, sc_vars
  - s3_path_hydatl: '{s3_base}/hydroATLAS/hydroatlas_vars.parquet' # path to hydroatlas data formatted for hydrofabric. Required only if hydroatlas variables desired.
  - ha_vars:  # hydroatlas variables. Must specify s3_path_hydatl if desired.
    - 'pet_mm_s01'
    - 'cly_pc_sav'
    - 'cly_pc_uav'
    - 'ari_ix_sav'
  - usgs_vars: # list of variables retrievable using nhdplusTools::get_characteristics_metadata().
    - 'TOT_TWI'
    - 'TOT_PRSNOW'
    - 'TOT_POPDENS90'
    - 'TOT_EWT'
    - 'TOT_RECHG'
    - 'TOT_PPT7100_ANN'
    - 'TOT_AET'
    - 'TOT_PET'
    - 'TOT_SILTAVE'
    - 'TOT_BASIN_AREA'
    - 'TOT_BASIN_SLOPE'
    - 'TOT_ELEV_MEAN'
    - 'TOT_ELEV_MAX'
    - 'TOT_Intensity'
    - 'TOT_Wet'
    - 'TOT_Dry'
  - sc_vars: # Streamcat variables of interest. #TODO add streamcat grabber capability to proc.attr.hydfab
    - # In this example case, no streamcat variables selected
